{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9112def0-8e54-4599-a5b9-ff0ed6b5f2d8",
   "metadata": {},
   "source": [
    "# Capstone Project EDA - Auxiliary Functions \n",
    "### Author: Hugo C Marrochio\n",
    "### Date: Dec 8th 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d091d-71e7-4278-8672-04a6251163ff",
   "metadata": {},
   "source": [
    "run on terminal \n",
    "\n",
    "jupyter nbconvert --to script auxiliary_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e278de-3d0a-4822-8dfb-d816e361ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #setting figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94e265-e999-4bb4-b003-700005f6c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #setting figure size\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from keras.layers import LSTM, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from keras.initializers import HeNormal\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc0a39-6da3-4646-b9b9-d7e7dedc88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210f8f4f-8e4f-444e-a84d-5832d5c3b4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1234\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set seeds for Python, NumPy, and TensorFlow\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(SEED)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# Set seeds for Python, NumPy, and TensorFlow\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "#tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a575e3-da49-4194-be2c-463146c10ec0",
   "metadata": {},
   "source": [
    "## 0.a - Loading all the relevant auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026b75c-f662-4ace-9e55-7f60b12f7d33",
   "metadata": {},
   "source": [
    "Function to set a random seet for the notebook. For now I do not need to call it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109736fc-acca-4a1c-80dc-97ee35756d84",
   "metadata": {},
   "source": [
    "#Set random seed, define a function so we don't need to recall later in the notebook\n",
    "def set_global_seed(seed=10):\n",
    "    '''\n",
    "    Set a global seed value for the notebook. If seed=10, you can just run set_global_seed()\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e758-e92f-4947-8711-f29b136aadbd",
   "metadata": {},
   "source": [
    "Here are some auxiliary functions for plotly animation. One simply separates the data in the dataframe in order to heatmap correlation matrix. The second is an argument for the duration of the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85735c4d-aead-4ad4-b187-a23b57c04c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dictionary with the data from the correlation matrices\n",
    "def df_to_plotly(df):\n",
    "    '''\n",
    "    Simple auxiliary function breaking down the parameters of the DataFrame in order to create a plotly heatmap.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    df: The Pandas DataFrame we want to plot a heatmap\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    z: values of df\n",
    "    x: column of df\n",
    "    y: index of df\n",
    "    \n",
    "    '''\n",
    "    return {'z': df.values.tolist(),\n",
    "            'x': df.columns.tolist(),\n",
    "            'y': df.index.tolist()}\n",
    "# Define frame arguments for the play button\n",
    "def frame_args(duration):\n",
    "    '''\n",
    "    Auxiliary function for the duration of frame in the animation.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    duration: Duration for a frame in the animation (I believe in miliseconds)\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    Dictionary with formated instructions for plotly function\n",
    "    \n",
    "    '''\n",
    "    return {\n",
    "        \"frame\": {\"duration\": duration, \"redraw\": True},\n",
    "        \"mode\": \"immediate\",\n",
    "        \"fromcurrent\": True,\n",
    "        \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6c069-1bd3-4b81-b390-a468ac9d856c",
   "metadata": {},
   "source": [
    "This is an auxiliary function to print the correct date ranges used to calculate the correlation matrices for the plotly animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2f6efa-57f1-4e66-a8ea-45524855efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_legend(df_og, row_step=15):\n",
    "    '''\n",
    "    Auxiliary function for heatmap animation plotting. It generates a list with dates such that shows the end date at each frame.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    df_og: The Pandas DataFrame we want to analyze. Notice this is in the original orientation, where there is a `Symbol` column.\n",
    "    \n",
    "    row_step: What is the interval we are adding to the correlations at each frame. Default is 15.\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    list_range_dates = List of strings representing the end date for the interval being calculated at each frame. Used in the animated heatmap.\n",
    "    '''\n",
    "    \n",
    "    #Generate just one stock for simplicity\n",
    "    examp=df_og['Symbol'].iloc[0]\n",
    "    df_og_ex=df_og[df_og['Symbol']==examp]\n",
    "\n",
    "    # Let us create a list to put the correct date range in our plot\n",
    "    list_range_dates=[]\n",
    "    start_date=f'{df_og_ex.iloc[0,0].year}-{df_og_ex.iloc[0,0].month}-{df_og_ex.iloc[0,0].day}'\n",
    "    for i in range (df_og_ex.shape[0] // row_step + 1):\n",
    "        end_date=f'{df_og_ex.iloc[i*row_step,0].year}-{df_og_ex.iloc[i*row_step,0].month}-{df_og_ex.iloc[i*row_step,0].day}'\n",
    "        list_range_dates.append(start_date + ' to ' + end_date)\n",
    "    return list_range_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd153f-5a3e-410e-8448-e5dba8262537",
   "metadata": {},
   "source": [
    "This function creates an animation where each frame is a correlation matrix calculated for a specific range - in the default case adding 15 data point each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2f2929-1ea7-4859-8d6e-63a5cae63ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function as a plotly object in order to plot the animation of Heatmap\n",
    "\n",
    "def plot_heatmap_animation(df_og, title_plot, row_step=15):\n",
    "    '''\n",
    "    Auxiliary function in order to animate a heatmap in Plotly. \n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    df_og: The Pandas DataFrame we want to analyze. Notice this is in the original orientation, where there is a `Symbol` column.\n",
    "    \n",
    "    title_plot: A string that simply labels the animation plot. Generally want to describe the properties of the DataFrame being plotted.\n",
    "    \n",
    "    row_step: What is the interval we are adding to the correlations at each frame. Default is 15.\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    fig: A Plotly object, we can simply fig.show() after calling the function. We can also save it to html.\n",
    "    '''\n",
    "    \n",
    "    # Manipulate the DataFrame such that each column corresponds to a stock. It facilitates the calculation of correlation.\n",
    "\n",
    "    grouped_value=df_og.groupby('Symbol')['Return'].apply(list).reset_index()\n",
    "    df_value=pd.DataFrame(grouped_value['Return'].tolist(), index=grouped_value['Symbol']).T\n",
    "\n",
    "\n",
    "    \n",
    "    # Define the row step size (e.g., every 15 trading days is the default)\n",
    "    frames = []\n",
    "\n",
    "    # Create correlation matrices for increasing number of rows\n",
    "    for i in range(1, df_value.shape[0] // row_step + 1):\n",
    "        corr_matrix = df_value[:i * row_step].corr()  # Compute correlation for i*row_step rows starting from 0\n",
    "        frames.append(corr_matrix)\n",
    "    n_frames=len(frames)\n",
    "    \n",
    "    #Get the list of dates, used for the legend at each frame of the animation\n",
    "    list_range_dates=dates_legend(df_og,row_step)\n",
    "\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for the initial frame\n",
    "    for step in np.arange(0, n_frames, 1):\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(df_to_plotly(frames[step]), colorscale='RdBu', name=\"t = \" + str(step), zmin=-1, zmax=1)\n",
    "        )\n",
    "    \n",
    "    # Ensure only the first trace is visible initially\n",
    "    for trace in fig.data:\n",
    "        trace.visible = False\n",
    "    fig.data[0].visible = True  # First trace is visible initially\n",
    "\n",
    "\n",
    "\n",
    "    # Create frames for the animation\n",
    "    fig.frames = [\n",
    "        go.Frame(data=[go.Heatmap(df_to_plotly(frames[k]), colorscale='RdBu', zmin=-1, zmax=1)],\n",
    "                 name=str(k)) for k in range(n_frames)\n",
    "    ]\n",
    "\n",
    "    # Define sliders for manual frame control\n",
    "    sliders = [\n",
    "        {\n",
    "            \"pad\": {\"b\": 10, \"t\": 60},\n",
    "            \"len\": 0.9,\n",
    "            \"x\": 0.1,\n",
    "            \"y\": 0,\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"args\": [[str(k)], frame_args(0)],\n",
    "                    \"label\": list_range_dates[k],\n",
    "                    \"method\": \"animate\",\n",
    "                }\n",
    "                for k in range(n_frames)\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Layout with play/pause buttons and sliders\n",
    "    fig.update_layout(\n",
    "        title=title_plot,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        updatemenus=[{\n",
    "            \"buttons\": [\n",
    "                {\n",
    "                    \"args\": [None, frame_args(100)],  # Play button\n",
    "                    \"label\": \"&#9654;\",  # play symbol\n",
    "                    \"method\": \"animate\",\n",
    "                },\n",
    "                {\n",
    "                    \"args\": [[None], frame_args(0)],  # Pause button\n",
    "                    \"label\": \"&#9724;\",  # pause symbol\n",
    "                    \"method\": \"animate\",\n",
    "                },\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"pad\": {\"r\": 10, \"t\": 70},\n",
    "            \"type\": \"buttons\",\n",
    "            \"x\": 0.1,\n",
    "            \"y\": 0,\n",
    "        }],\n",
    "        sliders=sliders\n",
    "    )\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3676d-0bcd-4a47-98f1-c79a8e9c16e5",
   "metadata": {},
   "source": [
    "Perform Kernel Density Estimator to the histogram \"data\". Adapted from Lopez Prado's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ad664a-86e4-47bf-97a9-880836b6a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KDE from the data\n",
    "def KDE_pdf(data, x_range, bandwidth=0.2):\n",
    "    \"\"\"\n",
    "    Perform Kernel Density Estimator for the data.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    data: In our case the data is the list of eigenvalues calculate from a correlation matrix.\n",
    "    \n",
    "    bandwidth: Parameter that control the smoothing of the KDE.\n",
    "\n",
    "    x_range: Range in x for evaluation of the KDE\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    pdf: It outputs the estimated pdf function from our data\n",
    "\n",
    "    \"\"\"\n",
    "    # Reshape data \n",
    "    data = data.reshape(-1, 1)\n",
    "    \n",
    "    # Use KernelDensity model from sklearn to fit the data\n",
    "    \n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(data)\n",
    "    \n",
    "    \n",
    "    # Calculate the probability, need to exponentiate to be a pdf\n",
    "    logProb = kde.score_samples(x_range)\n",
    "    pdf = pd.Series(np.exp(logProb), index=x_range.flatten())\n",
    "    \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582d99b-3c6f-4798-9089-ac1df2c4fc0a",
   "metadata": {},
   "source": [
    "Shifted Marcenko-Pastur distribution. Shifted in the sense that the range is from (0 to $\\lambda_+-\\lambda_-$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47685be7-314e-4661-8ed7-e8f3716cd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_MP_pdf_shift(q, sigma, pts=1000):\n",
    "    \"\"\"\n",
    "    Generate the theoretical Marchenko-Pastur PDF with shifted eigenvalues.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    q: Aspect ratio (T/N)\n",
    "    sigma: Variance (to be optimized)\n",
    "    pts: Number of points for the PDF.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    f: MP pdf as a pandas Series\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    lambda_max = (sigma**2) * (1 + np.sqrt(1/q))**2\n",
    "    lambda_min = (sigma**2) * (1 - np.sqrt(1/q))**2\n",
    "    lambda_vals = np.linspace(lambda_min + 10.**-14, lambda_max, pts)\n",
    "    lambda_vals_shift=np.linspace(10.**-14, lambda_max-lambda_min, pts)\n",
    "    \n",
    "    # The Marchenko-Pastur PDF formula\n",
    "    f = q * np.sqrt((lambda_max - lambda_vals) * (lambda_vals - lambda_min)) / (lambda_vals * 2 * np.pi * (sigma)**2)\n",
    "    \n",
    "    # Return as a pandas Series with lambda_vals as index\n",
    "    f = pd.Series(f, index=lambda_vals_shift)\n",
    "    return f\n",
    "\n",
    "def f_MP_pdf_plot(q, sigma, pts=1000,shift=True):\n",
    "    \"\"\"\n",
    "    Generate the theoretical Marchenko-Pastur PDF (default is with shifted eigenvalues), return both axis for plotting\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    q: Aspect ratio (T/N)\n",
    "    sigma: Variance (to be optimized)\n",
    "    pts: Number of points for the PDF.\n",
    "    shift: A flag set to True. If we need to plot the MP pdf\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    f: MP pdf\n",
    "    lambda_vals_shift: x range of values\n",
    "    \n",
    "    \"\"\"\n",
    "    lambda_max = (sigma**2) * (1 + np.sqrt(1/q))**2\n",
    "    lambda_min = (sigma**2) * (1 - np.sqrt(1/q))**2\n",
    "    lambda_vals = np.linspace(lambda_min + 10.**-14, lambda_max, pts)\n",
    "    lambda_vals_shift = np.linspace( 10.**-14, lambda_max-lambda_min, pts)\n",
    "    \n",
    "    # The Marchenko-Pastur PDF formula\n",
    "    f = q * np.sqrt((lambda_max - lambda_vals) * (lambda_vals - lambda_min)) / (lambda_vals * 2 * np.pi * (sigma)**2)\n",
    "\n",
    "    #If flag shift is false, we return the unshifted value\n",
    "    if shift==False:\n",
    "        return f,lambda_vals\n",
    "    \n",
    "    # Return as a pandas Series with lambda_vals as index\n",
    "    #f = pd.Series(f, index=lambda_vals)\n",
    "    return f,lambda_vals_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2bf76-ab3f-4544-8f13-f42f927699f9",
   "metadata": {},
   "source": [
    "Cost function for the minimization. We will compare the MP-pdf with the kde of the histogram of the eigenvalues.  Adapted from Lopez Prado's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa2156d-9e89-4233-8eec-bf187424469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(params, eVal, bWidth,flag=False):\n",
    "    \"\"\"\n",
    "    Cost function for the minimization between data and MP pdfs.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    params: array containing both parameters being optimized, q and sigma.\n",
    "    \n",
    "    eval: The eigenvalue list used to calculate KDE and the data_pdf.\n",
    "\n",
    "    bWidth: Parameter determining the \"smoothing\" of the KDE procedure.\n",
    "\n",
    "    flag: Turn to True if we want to print the convergence of the sum of squared during the minimization.\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    sse: Sum of squared errors between the theoretical prediction MP_pdf and the pdf obtained by the data data_pdf.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # The parameters being optimized\n",
    "    sigma = params[0]  \n",
    "    q= params[1]\n",
    "\n",
    "    \n",
    "    # Generate the theoretical MP_pdf using f_MP_pdf_shift\n",
    "    MP_pdf = f_MP_pdf_shift(q, sigma)\n",
    "    \n",
    "    # Generate data_pdf using KDE_pdf\n",
    "    data_pdf = KDE_pdf(eVal, x_range=MP_pdf.index.values.reshape(-1, 1),bandwidth=bWidth)\n",
    "    \n",
    "    # Compute the sum of squared errors (SSE) \n",
    "    sse = np.sum((MP_pdf - data_pdf) ** 2)\n",
    "\n",
    "    # Passing a flag, set to True if we want to print the SSE at each step\n",
    "    if flag:\n",
    "        print(f\"SSE: {sse}\")\n",
    "    \n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a695322-2892-4b3b-9fcb-f59398fe5265",
   "metadata": {},
   "source": [
    "Minimization procedure to find best q and $\\sigma$ to fit the kde of the correlation matrix histogram.  Adapted from Lopez Prado's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ef5009-5561-40c3-acdb-5973c99d23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxEval(eVal, bWidth,flag=False):\n",
    "    \"\"\"\n",
    "    Fit shifted Marcenko-Pastur pdf to the data.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    eVal: Data, in our case a list of eigenvalues calculated from a correlation matrix.\n",
    "    \n",
    "    bWidth: Parameter determining the \"smoothing\" of the KDE procedure.\n",
    "\n",
    "    flag: Turn to True if we want to print the convergence of the sum of squared during the minimization.\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    eMax: Scaled maximum random eigenvalue predicted by RMT and the MP pdf.\n",
    "    \n",
    "    sigma_opt = Resulting sigma from optimization. \n",
    "    \n",
    "    q_opt = Resulting q from optimization. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guess for the parameters being optimized\n",
    "    initial_params = np.array([0.99,40])\n",
    "\n",
    "    # Minimize cost_function defined previously. Make sure we put bounds for the parameters,\n",
    "    \n",
    "    result = minimize(cost_function, initial_params, args=(eVal, bWidth,flag),\n",
    "                      bounds=[(1E-5, 1-1E-5),(4 + 1E-5, 50)])\n",
    "    \n",
    "    # Print the optimization result\n",
    "    print(f\"Optimization result: {result}\")\n",
    "    \n",
    "    # If optimization succeeds, use the optimized sigma\n",
    "    if result.success:\n",
    "        sigma_opt = result.x[0]\n",
    "        q_opt=result.x[1]\n",
    "    else:\n",
    "        sigma_opt = 1  # Fallback in case of failure\n",
    "        q_opt=10\n",
    "    \n",
    "    # Calculate the maximum eigenvalue based on the optimized sigma, as described by Lopez Prado\n",
    "    eMax = sigma_opt * (1 + np.sqrt(1/q_opt)) ** 2\n",
    "    \n",
    "    return eMax, sigma_opt, q_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccbc60-e308-49e9-9ccc-fffd48492de8",
   "metadata": {},
   "source": [
    "Function to __sample__ random correlation matrices and return all the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5aeabd-bbbb-4ecb-91cc-31a55aa775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cor_matrix(T,N,n_sample, sigma=1):\n",
    "    '''\n",
    "    Samples random covariance matrice, calculates the eigenvalues\n",
    "    and returns them as a list.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    T: Dimension of the Original matrix X\n",
    "    N: Dimension of the Original matrix X\n",
    "    n_sample: The amouth of matrices from GOE that we will sample over\n",
    "    sigma: Standard deviation of each element. If not provided, default is sigma=1\n",
    "\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    eig_list: List of total eigenvalues for all the sampled matrices\n",
    "    '''\n",
    "    eig_list=[]\n",
    "\n",
    "    #Sample eigevanlues for different n_sample \n",
    "    for i in range(n_sample):\n",
    "        # Generate random rectangular matrix\n",
    "        M=sigma*np.random.randn(T, N)\n",
    "        \n",
    "        # Calculate correlation matrix (normalized with T for the eigenvalue distribution)\n",
    "        C=np.matmul(M.T,M)/T\n",
    "        \n",
    "        #Calculate the eigenvalues\n",
    "        eig=np.linalg.eigvalsh(C)\n",
    "        eig_list.extend(eig)\n",
    "    return eig_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a52b9-62b1-49ee-832c-efa8f2b94685",
   "metadata": {},
   "source": [
    "### Useful Denoising functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c58ee-ef9a-42ff-9217-d939c2bc7dc1",
   "metadata": {},
   "source": [
    "Return Eigenvector matrix and eigenvalue such that a Hermitian Matrix decomposes as $W \\Lambda W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f448db-08c8-4106-ad4f-50e7c667386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the eigenvector matrix and the eigenvalues\n",
    "def eig_dec(M):\n",
    "    '''\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    M: Symmetric matrix M\n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    Lambda: Diagonal matrix with diagonal elements representing the eigenvalues in descending order\n",
    "    \n",
    "    eigVec: Matrix with eigenctors associate with the eigenvalues in Lambda\n",
    "    \n",
    "    '''\n",
    "    eigVal, eigVec = np.linalg.eigh(M)\n",
    "    \n",
    "    #eigenvalues are returned in ascending order! In order to agree with the order of np.linalg.eigvals, let us reverse it\n",
    "\n",
    "    # argsort() returns the indexes that would sort in ascending order, reverse for descending\n",
    "\n",
    "    index=eigVal.argsort()[::-1]\n",
    "    eigVal=eigVal[index]\n",
    "    eigVec=eigVec[:,index]\n",
    "    \n",
    "    Lambda = np.diag(eigVal)\n",
    "\n",
    "    return Lambda, eigVec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd20122-1e66-4da9-9c36-7908b50c7499",
   "metadata": {},
   "source": [
    "Denoise procedure. Given the list of eigenvalues, eigenvector and the number of eigenvalues that are not associated to the random range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808f2c3e-52cd-4ade-a10c-6c66a7aefe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_eig(eigVal,eigVec,n_random):\n",
    "    '''\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    eigVal: Eigenvalue matrix, can be for instance the output of eig_dec()\n",
    "\n",
    "    eigVec: Eigenvector matrix, can be for instance the output of eig_dec()\n",
    "\n",
    "    n_random: number of eigenvalues OUTSIDE the range of randomness.\n",
    "\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    C_norm: Symmetric matrix where the eigenvalues in the random range were regularized.\n",
    "    \n",
    "    '''\n",
    "    eigVal_den=np.diag(eigVal).copy()\n",
    "    n_total=eigVal_den.shape[0] #total size of the array of eigenvalues\n",
    "\n",
    "    #perform the denoising for eigenvalues in the random range. Notice that descending order is important\n",
    "    eigVal_den[n_random:]=eigVal_den[n_random:].sum()/(n_total-n_random)\n",
    "    eigVal_den=np.diag(eigVal_den)\n",
    "\n",
    "    #Use @ instead of np.matmult for cleaner code\n",
    "\n",
    "    print(f'Shape of eigVal is {eigVal.shape}')\n",
    "    print(f'Shape of eigVal_den is {eigVal_den.shape}')\n",
    "    print(f'Shape of eigVec is {eigVec.shape}')\n",
    "    \n",
    "    C= eigVec @ eigVal_den @ eigVec.T\n",
    "\n",
    "    print(f'Shape of C is {C.shape}')\n",
    "\n",
    "    std=np.sqrt(np.diag(C))\n",
    "    \n",
    "    print(f'Shape of std is {std.shape}')\n",
    "\n",
    "    print(f'Shape of outer is {np.outer(std,std).shape}')\n",
    "    \n",
    "    \n",
    "    C_norm=C/np.outer(std,std)\n",
    "    C_norm[C_norm<-1],C_norm[C_norm>1]=-1,1\n",
    "    return C_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65994285-6d5e-4e68-b19b-c7ca91dd9e56",
   "metadata": {},
   "source": [
    "# Price Forecasting auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de33b9-221d-4172-9bb0-e20749c807b2",
   "metadata": {},
   "source": [
    "This function is useful in order to return scaled arrays, with respect to the training data. It is prudent to rescale the \n",
    "data in order to train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1147ea0-b7b2-4308-bb33-4d3b911f5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_array(df_train,df_test,key='Adj Close'):\n",
    "    '''\n",
    "    This function takes two dataframes to scale, if no other key given it assumes price 'Adj Close' is the column being analyze.\n",
    "\n",
    "    It returns np scaled arrays between 1 and 0 with shape (n_rows,1).\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    df_train: Pandas DataFrame, the data we fit MinMaxScaler.\n",
    "\n",
    "    df_test: Pandas DataFrame, the data we rescale based on train data's fit.\n",
    "    \n",
    "    key: If not specified, analyze price 'Adj Close'\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    scaled_array_train: Scaled train data, fit to be between 0 and 1.\n",
    "    \n",
    "    scaled_array_test: Scaled test array of shape (n_rows,1) using the fit of train data. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    array_train=df_train[key].values\n",
    "    array_test=df_test[key].values\n",
    "\n",
    "    \n",
    "    #Reshape\n",
    "\n",
    "    array_train=array_train.reshape(-1,1)\n",
    "    array_test=array_test.reshape(-1,1)\n",
    "\n",
    "    #Scale the data, fit based on training\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_array_train = scaler.fit_transform(array_train)\n",
    "\n",
    "    #Also scale the test, but only transform with scaler\n",
    "    scaled_array_test = scaler.transform(array_test)\n",
    "\n",
    "    return scaled_array_train, scaled_array_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ced2b-b884-4e7f-abf6-542ae8db2fe8",
   "metadata": {},
   "source": [
    "Since the prediction will be given scaled, we want to write a function to transform back to the original prices scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcccadc-8566-4495-a7a6-1a3447818167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_inverse_array(df_train,array_target,key='Adj Close'):\n",
    "    '''\n",
    "    This function takes array_target as an input, shape (n_Array,1), and scales back to the original df_train[key] scale.\n",
    "    One simple use is to scale back the NN prediction to the original prices scale.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    df_train: Pandas DataFrame, the data we used to fit MinMaxScaler.\n",
    "\n",
    "    array_target: Array in the range given by MinMaxScaler, use this function to rescale it back to original values.\n",
    "\n",
    "    \n",
    "    key: If not specified, analyze price 'Adj Close'\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    unscaled_array_target: Using the MinMaxScaler fit, take array_target and inverse back to the original scale of df_train[key].\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    array_train=df_train[key].values\n",
    "\n",
    "    \n",
    "    #Reshape\n",
    "\n",
    "    array_train=array_train.reshape(-1,1)\n",
    "\n",
    "    #Scale the data, fit based on training\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_array_train = scaler.fit_transform(array_train)\n",
    "\n",
    "    #Also scale the test, but only transform with scaler\n",
    "    unscaled_array_target = scaler.inverse_transform(array_target)\n",
    "\n",
    "    return unscaled_array_target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75edfe-0b54-4366-a15a-b6a8c11e2224",
   "metadata": {},
   "source": [
    "We want to produce smaller portions of the data timeseries to train a learning algorithm.\n",
    "\n",
    "Consider this example of rolling window of size $5$.\n",
    "\n",
    "Suppose a time series of interest is $[ a_1,a_2, a_3, a_4, a_5, a_6, a_7, a_8]$.\n",
    "\n",
    "__Step 1:__\n",
    "\n",
    "- rolling window:$X_1=[ a_1,a_2, a_3, a_4, a_5]$, \n",
    "\n",
    "- target is $y_1=[a_6]$.\n",
    "\n",
    "\n",
    "__Step 2:__\n",
    "\n",
    "- rolling window:$X_2=[ a_2, a_3, a_4, a_5,a_6]$, \n",
    "\n",
    "- Target is $y_2=[a_7]$.\n",
    "\n",
    "\n",
    "\n",
    "__Step 3:__\n",
    "\n",
    "- rolling window:$X_3=[ a_3, a_4, a_5, a_6, a_7]$, \n",
    "\n",
    "- Target is $y_3=[a_8]$.\n",
    "\n",
    "\n",
    "Therefore, the feature input would be the $X_{\\text{input}} = (X_1,X_2,X_3) $ and the target would be $y_{\\text{input}} = (y_1,y_2,y_3)$. \n",
    "The function adds one more dimension in order to be ready for a Keras training:\n",
    "\n",
    "- X shape: (n_input-window, window, 1)\n",
    "- y shape: (n_input-window, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a0234-3ca7-463f-b042-422e06a8bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_time_series(array,window):\n",
    "    '''\n",
    "    Create a rolling window array.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    array: Time series that we are converting into a rolling window format. Assumed to contain n_input datapoints >  window.\n",
    "\n",
    "    window: The range to be converted into an input X, with the next point being the target y.\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    X: Numpy array datapoint in a rolling window format. Shape is (n_input-window, window, 1).\n",
    "\n",
    "    y: The next point in the time series. The target for each rolling window of size window. Shape is (n_input-window, 1).\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if window>len(array):\n",
    "        return print(f'Cannot create a window of size {window} bigger than the data points provided of size {len(array)}')\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(window, len(array)):\n",
    "        X.append(array[i-window:i, 0])\n",
    "        y.append(array[i, 0])\n",
    "        \n",
    "    #Transform to numpy\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    #Reshaping for correct input for NN training\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1],1))\n",
    "    y = np.reshape(y, (y.shape[0],1))\n",
    "\n",
    "\n",
    "    \n",
    "    return X,y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ec81f-f268-485e-851e-b6583797bbb3",
   "metadata": {},
   "source": [
    "Here we define our functions in order to train neural networks. We define separately RNN, GRU and LSTM for clarity.\n",
    "\n",
    "We defined the default value for the best hyperparameters we encountered in our investigation, for ease of application. However, we still\n",
    "passed these as input to be able investigate other hyperparameter values as needed.\n",
    "\n",
    "We __fix a seed__ for reproducibility.\n",
    "\n",
    "For best practices, we add Tensorboard and ModelCheckpoint in our model. Since we are not training on a huge amount of data, it is not so important for our analysis. However, I thought it would be a useful learning tool for future projects. In addition, saving the model every few epochs __is__ useful for the Reinforcement Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e006b-a0c6-454e-8402-ab36977c6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_run_RNN(X_train, y_train, experiment,hidden_states=80, activation='ReLU',dropout=0.05,flag_optimizer=True,optimizer='Adam',learning_rate=0.001,batch_size=2, epoch=20):\n",
    "    '''\n",
    "    The function to train and fit neural networks! Created the base example the architecture that was mostly successful \n",
    "    for all architectures, but still put enough of hyperparameters as optional inputs so we can experiment with them. \n",
    "\n",
    "    Use simple Recurrent Neural Network as architecture!\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    X_train: Shape (n_input-window,window,1), the training data for the neural network.\n",
    "\n",
    "    y_train: Shape (n_input-window,1), the value at window+1 used as a target for training.\n",
    "\n",
    "    experiment: String describing the experiment, useful for labeling the logs. For instance, for a hidden_states experiment, call\n",
    "    the f-string f'hiddenlayers_{hidden_states}'\n",
    "\n",
    "    hidden_states: We fix the architecture as 40-hidden_states-40-1, default is hidden_states=80.\n",
    "\n",
    "    activation: Activation function for the 40-hidden_states-40 layers, default is ReLU.\n",
    "\n",
    "    dropout: Dropout of nodes, useful to control overfitting when it is important. Default is dropout=0.05.\n",
    "\n",
    "    flag_optimizer: Default is True. Pass False if you want to include an optimizer with more options than learning_rate.\n",
    "\n",
    "    optimizer: Default is Adam\n",
    "\n",
    "    learning_rate: Learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "    batch_size: Default is 2.\n",
    "\n",
    "    epoch: Default is 20.\n",
    "       \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    model: Keras object with the tensor network trained on (X_train,y_train).\n",
    "\n",
    "    history: function from the Keras fit, useful for accessing the loss.\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    \n",
    "    print(f'Working on experiment {experiment}')\n",
    "\n",
    "    # Fix seed for reproducibility\n",
    "    \n",
    "    tf.keras.utils.set_random_seed(1234)\n",
    "\n",
    "    # Here we save the logs of the run in Tensorboard.\n",
    "    \n",
    "    log_dir = f\"logs/fit/RNN_{experiment}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "\n",
    "    # Save in .h5 file every 10 epoch using ModelCheckpoint, function of the dimension of X_train and batch size.\n",
    "    \n",
    "    save_freq_ex=int(10*(X_train.shape[0])/batch_size)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f'model/RNN_{experiment}_epoch_{{epoch:02d}}.h5',  \n",
    "    save_freq=save_freq_ex,                                 \n",
    "    save_best_only=False,                      \n",
    "    save_weights_only=False,                   \n",
    "    verbose=0                                  \n",
    "    )\n",
    "\n",
    "    # Put together callbacks for EarlyStopping, TensorBoard and ModelCheckpoint\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='loss', patience=4), tensorboard_callback,checkpoint_callback]\n",
    "\n",
    "    \n",
    "    # Construct the structure of the network\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding layers, fixed the first one to be the same amount as the rolling window\n",
    "    \n",
    "    model.add(SimpleRNN(units = 40, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True,\n",
    "                        input_shape = (X_train.shape[1],1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Middle layer, can change the size of hidden_states\n",
    "    \n",
    "    model.add(SimpleRNN(units = hidden_states, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True))\n",
    "\n",
    "    # Fixed final layer to have same size as the first one.\n",
    "    \n",
    "    model.add(SimpleRNN(units = 40,activation = activation))\n",
    "    \n",
    "    #Output Layers\n",
    "    \n",
    "    model.add(Dense(units = 1,activation='ReLU')) # One prediction into the future! \n",
    "\n",
    "\n",
    "    # To compile, we are giving two different possibilities: One is that optimizer is just a string,\n",
    "    # and we only change learning_rate. If we need an optimizer with more options, pass flag_optimizer=False.\n",
    "    \n",
    "    if flag_optimizer:\n",
    "        model.compile(optimizer = eval(optimizer)(learning_rate=learning_rate), \n",
    "                        loss = \"mean_squared_error\")\n",
    "    else:\n",
    "        model.compile(optimizer = eval(optimizer), \n",
    "                        loss = \"mean_squared_error\")\n",
    "        \n",
    "        \n",
    "    # Train the model!\n",
    "    \n",
    "    model_history=model.fit(X_train, y_train, epochs = epoch, batch_size = batch_size,callbacks=[callbacks],verbose=0)\n",
    "\n",
    "    # Return the model itself, as well as model_history (which contains information about loss).\n",
    "\n",
    "    return model, model_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15c8782-514d-4653-8984-dec84425177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "def neural_network_run_GRU(X_train, y_train, experiment,hidden_states=80, activation='ReLU',dropout=0.05,flag_optimizer=True,optimizer='Adam',learning_rate=0.001,batch_size=2, epoch=20):\n",
    "    '''\n",
    "    The function to train and fit neural networks! Created the base example the architecture that was mostly successful \n",
    "    for all architectures, but still put enough o hyperparameters as optional inputs so we can experiment with them. \n",
    "\n",
    "    Use GRU as base architecture!\n",
    "\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    X_train: Shape (n_input-window,window,1), the training data for the neural network.\n",
    "\n",
    "    y_train: Shape (n_input-window,1), the value at window+1 used as a target for training.\n",
    "\n",
    "    experiment: String describing the experiment, useful for labeling the logs. For instance, for a hidden_states experiment, call\n",
    "    the f-string f'hiddenlayers_{hidden_states}'\n",
    "\n",
    "    hidden_states: We fix the architecture as 40-hidden_states-40-1, default is hidden_states=80.\n",
    "\n",
    "    activation: Activation function for the 40-hidden_states-40 layers, default is ReLU.\n",
    "\n",
    "    dropout: Dropout of nodes, useful to control overfitting when it is important. Default is dropout=0.05.\n",
    "\n",
    "    flag_optimizer: Default is True. Pass False if you want to include an optimixer with more options than learning_rate.\n",
    "\n",
    "    optimizer: Default is Adam\n",
    "\n",
    "    learning_rate: Learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "    batch_size: Default is 2.\n",
    "\n",
    "    epoch: Default is 20.\n",
    "       \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    model: Keras object with the tensor network trained on (X_train,y_train).\n",
    "\n",
    "    history: function from the Keras fit, useful for accessing the loss.\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    print(f'Working on experiment {experiment}')\n",
    "    tf.keras.utils.set_random_seed(1234)\n",
    "\n",
    "    # Save every 10 epoch, function of the dimension of X_train and batch size.\n",
    "    \n",
    "    log_dir = f\"logs/fit/GRU_{experiment}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "\n",
    "    # Save every 10 epoch, function of the dimension of X_train and batch size.\n",
    "    \n",
    "    save_freq_ex=int(10*(X_train.shape[0])/batch_size)\n",
    "\n",
    "    # Save in .h5 file every 10 epoch using ModelCheckpoint, function of the dimension of X_train and batch size.\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f'model/GRU_{experiment}_epoch_{{epoch:02d}}.h5',  \n",
    "    save_freq=save_freq_ex,\n",
    "    #save_freq='epoch',                         \n",
    "    #period=10,                                 \n",
    "    save_best_only=False,                      \n",
    "    save_weights_only=False,                   \n",
    "    verbose=0                                  \n",
    "    )\n",
    "\n",
    "    # Put together callbacks for EarlyStopping, TensorBoard and ModelCheckpoint\n",
    "\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='loss', patience=4), tensorboard_callback,checkpoint_callback]\n",
    "\n",
    "    \n",
    "    # Construct the structure of the network\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding layers, fixed the first one to be the same amount as the rolling window\n",
    "\n",
    "    model.add(GRU(units = 40, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True,\n",
    "                        input_shape = (X_train.shape[1],1)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    \n",
    "    # Middle layer, can change the size of hidden_states\n",
    "\n",
    "    model.add(GRU(units = hidden_states, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True))\n",
    "    \n",
    "    # Fixed final layer to have same size as the first one.\n",
    "\n",
    "    \n",
    "    model.add(GRU(units = 40,activation = activation))\n",
    "    \n",
    "    #Output Layer\n",
    "    \n",
    "    model.add(Dense(units = 1,activation='ReLU')) # One prediction into the future\n",
    "\n",
    "    # To compile, we are giving two different possibilities: One is that optimizer is just a string,\n",
    "    # and we only change learning_rate. If we need an optimizer with more options, pass flag_optimizer=False.\n",
    "\n",
    "    if flag_optimizer:\n",
    "        model.compile(optimizer = eval(optimizer)(learning_rate=learning_rate), # change learning rate from 0.0005 to 0.00005\n",
    "                        loss = \"mean_squared_error\")\n",
    "    else:\n",
    "        model.compile(optimizer = eval(optimizer), # change learning rate from 0.0005 to 0.00005\n",
    "                        loss = \"mean_squared_error\")\n",
    "        \n",
    "        \n",
    "    # Train the model\n",
    "    model_history=model.fit(X_train, y_train, epochs = epoch, batch_size = batch_size,callbacks=[callbacks],verbose=0)\n",
    "\n",
    "    # Return the model itself, as well as model_history (which contains information about loss).\n",
    "\n",
    "    return model, model_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b622b-df11-44d8-9065-1df30cb977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_run_LSTM(X_train, y_train, experiment,hidden_states=80, activation='ReLU',dropout=0.05,flag_optimizer=True,optimizer='Adam',learning_rate=0.001,batch_size=2, epoch=20):\n",
    "    '''\n",
    "    The function to train and fit neural networks! Created the base example the architecture that was mostly successful \n",
    "    for all architectures, but still put enough o hyperparameters as optional inputs so we can experiment with them. \n",
    "\n",
    "    Use LSTM as base architecture!\n",
    "\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    X_train: Shape (n_input-window,window,1), the training data for the neural network.\n",
    "\n",
    "    y_train: Shape (n_input-window,1), the value at window+1 used as a target for training.\n",
    "\n",
    "    experiment: String describing the experiment, useful for labeling the logs. For instance, for a hidden_states experiment, call\n",
    "    the f-string f'hiddenlayers_{hidden_states}'\n",
    "\n",
    "    hidden_states: We fix the architecture as 40-hidden_states-40-1, default is hidden_states=80.\n",
    "\n",
    "    activation: Activation function for the 40-hidden_states-40 layers, default is ReLU.\n",
    "\n",
    "    dropout: Dropout of nodes, useful to control overfitting when it is important. Default is dropout=0.05.\n",
    "\n",
    "    flag_optimizer: Default is True. Pass False if you want to include an optimixer with more options than learning_rate.\n",
    "\n",
    "    optimizer: Default is Adam\n",
    "\n",
    "    learning_rate: Learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "    batch_size: Default is 2.\n",
    "\n",
    "    epoch: Default is 20.\n",
    "       \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    model: Keras object with the tensor network trained on (X_train,y_train).\n",
    "\n",
    "    history: function from the Keras fit, useful for accessing the loss.\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    print(f'Working on experiment {experiment}')\n",
    "    tf.keras.utils.set_random_seed(1234)\n",
    "\n",
    "    # Save every 10 epoch, function of the dimension of X_train and batch size.\n",
    "\n",
    "    \n",
    "    log_dir = f\"logs/fit/LSTM_{experiment}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "\n",
    "    # Save every 10 epoch, function of the dimension of X_train and batch size.\n",
    "    \n",
    "    save_freq_ex=int(10*(X_train.shape[0])/batch_size)\n",
    "\n",
    "    # Save in .h5 file every 10 epoch using ModelCheckpoint, function of the dimension of X_train and batch size.\n",
    "\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f'model/LSTM_{experiment}_epoch_{{epoch:02d}}.h5',  \n",
    "    save_freq=save_freq_ex,\n",
    "    #save_freq='epoch',                         \n",
    "    #period=10,                                 \n",
    "    save_best_only=False,                      \n",
    "    save_weights_only=False,                   \n",
    "    verbose=0                                  \n",
    "    )\n",
    "\n",
    "    # Put together callbacks for EarlyStopping, TensorBoard and ModelCheckpoint\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='loss', patience=4), tensorboard_callback,checkpoint_callback]\n",
    "\n",
    "    \n",
    "    # Construct the structure of the network\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding layers, fixed the first one to be the same amount as the rolling window\n",
    "\n",
    "    model.add(LSTM(units = 40, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True,\n",
    "                        input_shape = (X_train.shape[1],1)))\n",
    "    model.add(Dropout(dropout)) \n",
    "    \n",
    "    # Middle layer, can change the size of hidden_states\n",
    "\n",
    "    model.add(LSTM(units = hidden_states, \n",
    "                        activation = activation,\n",
    "                        return_sequences = True))\n",
    "    \n",
    "    # Fixed final layer to have same size as the first one.\n",
    "\n",
    "    model.add(LSTM(units = 40,activation = activation))\n",
    "    \n",
    "    #Output Layer\n",
    "    \n",
    "    model.add(Dense(units = 1,activation='ReLU')) # One prediction into the future\n",
    "\n",
    "    # To compile, we are giving two different possibilities: One is that optimizer is just a string,\n",
    "    # and we only change learning_rate. If we need an optimizer with more options, pass flag_optimizer=False.\n",
    "\n",
    "    if flag_optimizer:\n",
    "        model.compile(optimizer = eval(optimizer)(learning_rate=learning_rate), # change learning rate from 0.0005 to 0.00005\n",
    "                        loss = \"mean_squared_error\")\n",
    "    else:\n",
    "        model.compile(optimizer = eval(optimizer), # change learning rate from 0.0005 to 0.00005\n",
    "                        loss = \"mean_squared_error\")\n",
    "        \n",
    "        \n",
    "    # Train the model\n",
    "    model_history=model.fit(X_train, y_train, epochs = epoch, batch_size = batch_size,callbacks=[callbacks],verbose=0)\n",
    "\n",
    "    # Return the model itself, as well as model_history (which contains information about loss).\n",
    "\n",
    "    return model, model_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31aa581-a4cd-4132-bfd2-73852cac94a0",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61ebc1-bab5-436f-ba7b-1cfdd4f5dde9",
   "metadata": {},
   "source": [
    "First, we define a class for the ReplayBuffer. In reinforcement learning, a crucial part of the algorithm is revisiting older states, so we create a double-ended queue (deque) in order to efficiently store the states. We can efficiently modify either ends of the queue.\n",
    "\n",
    "Due to memory limitations, we try to keep the size of the buffer between 1000-2000 entries.\n",
    "\n",
    "Let us describe the important methods of the class:\n",
    "\n",
    "-  `__init__`: Create a deque with default size 2000 (we will create a smaller one in the main notebook).\n",
    "  \n",
    "-  `store`: Adds experience to the deque. For instance, experience can be a tuple of information about the transition the agent is taking,\n",
    "so say experience = (state, action, reward, next_state). If the list is full, older experiences will be deleted.\n",
    "\n",
    "-  `sample`: Sample a random batch of experiences in the deque.\n",
    "\n",
    "-  `size`: Returns how many experiences are stored.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24eeb874-41f8-4cec-96c0-f1d868375235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    '''\n",
    "    ReplayBuffer for storing and sampling experiences in reinforcement learning.\n",
    "\n",
    "    The ReplayBuffer is a double-ended queue (deque) used to efficiently store experiences of the agent \n",
    "    interacting with the environment. This enables revisiting older states to improve the stability of learning.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    max_size: The maximum number of experiences the buffer can hold. Defaults to 2000. \n",
    "              Older experiences are automatically removed when the buffer exceeds this size.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "\n",
    "    store: Adds an experience to the buffer. Experience is typically a tuple (state, action, reward, next_state).\n",
    "           If the buffer is full, the oldest experience is removed to make space.\n",
    "\n",
    "    sample: Returns a random batch of experiences from the buffer. The size of the batch is specified by the user.\n",
    "            This helps diversify training by exposing the agent to a variety of transitions.\n",
    "\n",
    "    size: Returns the current number of experiences stored in the buffer.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    None: The class itself does not return anything but provides methods to store and retrieve experiences. \n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, max_size=2000,seed=1234):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        #self.seed = seed\n",
    "        #random.seed(seed)\n",
    "\n",
    "    \n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #random.seed(self.seed)\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebceaaac-1401-43d2-8e9b-c11f931f1fc0",
   "metadata": {},
   "source": [
    "Next, we create a Q-network for reinforcement learning. Due to computational constraints, we do not create a very deep network, but it still enough to demonstrate the capabilities of the model.\n",
    "\n",
    "We essentially create a Keras Simple RNN network with\n",
    "\n",
    "- SimpleRNN with 16 units to process sequential data.\n",
    "- Dropout layer (rate 0.1) to reduce overfitting.\n",
    "- Dense layer with 8 units and ReLU activation.\n",
    "- Dense output layer with 3 units (actions, __Buy, Hold, Sell__) and linear activation.\n",
    "- Adam optimizer and MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafa13b-2e6f-4f65-8a6f-20aaff0c4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_network(window_size):\n",
    "    '''\n",
    "    Create a Q-Network for reinforcement learning.\n",
    "\n",
    "    This function builds a neural network using Keras to approximate the Q-function, \n",
    "    which predicts the expected rewards for actions (Buy, Hold, Sell) based on input features.\n",
    "    We pass a seed for reproducibility.\n",
    "    \n",
    "    The hyperparameters of the network is:\n",
    "        - SimpleRNN with 16 units to process sequential data.\n",
    "        - Dropout layer (rate 0.1) to reduce overfitting.\n",
    "        - Dense layer with 8 units and ReLU activation.\n",
    "        - Dense output layer with 3 units (actions, __Buy, Hold, Sell__) and linear activation.\n",
    "        - Adam optimizer and MSE loss.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    window_size: Number of past time steps used as input. The total input size is window_size + 1, \n",
    "                 including the predicted price.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    model: A compiled Keras model.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Seed for reproducibility\n",
    "    tf.keras.utils.set_random_seed(1234)\n",
    "    \n",
    "    input_features = window_size + 1  # Include predicted price as one of the input!\n",
    "\n",
    "    # Create the NN architecture\n",
    "    \n",
    "    model = Sequential([\n",
    "        SimpleRNN(16, input_shape=(input_features, 1), return_sequences=False),\n",
    "        Dropout(0.1),\n",
    "        \n",
    "        # Next layers\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(3, activation='linear')  # 3 actions: Buy, Hold, Sell\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001)\n",
    ", loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c354e-7649-42c9-a4e5-17b87563a504",
   "metadata": {},
   "source": [
    "Next, we either take an exploration action, which means a random experience, or we exploit the best action we have (based on Q-value). \n",
    "\n",
    "We define the following function, that takes epsilon to control the probability of choosing exploration (1-epsilon for exploitation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac6342-d080-4ee2-abcf-2e6c03c4e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_values, epsilon, seed=SEED):\n",
    "    '''\n",
    "    Choose an action using an epsilon-greedy strategy.\n",
    "\n",
    "    This function selects an action based on Q-values, balancing exploration and exploitation,\n",
    "    a key aspect of Q-learning.\n",
    "\n",
    "    Epsilon controls the probability it chooses a random exploring action. Otherwise, it selects\n",
    "    a greedy action (controlled by highest Q-value).\n",
    "   \n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    q_values: A list or array of Q-values for each action.\n",
    "\n",
    "    epsilon: A float (0 <= epsilon <= 1) representing the exploration probability. High epsilon favors exploration.\n",
    "           \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    action: An integer representing the chosen action:\n",
    "            - Random action (if exploring) or\n",
    "            - The index of the maximum Q-value (if exploiting).\n",
    "    '''\n",
    "    #np.random.seed(seed)\n",
    "\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(3)  # Random action, 3 since we have 3 possible actions\n",
    "    return np.argmax(q_values)  # Greedy action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a950150-8126-48d9-aaea-42678a59903d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1850c945-066e-43dc-9e43-1dcc67688c17",
   "metadata": {},
   "source": [
    "In reinforcement learning we want to stabilize training by keeping the target network fixed for a few steps while \n",
    "updating the Q-network. The next function updates the weights of the target when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b660c2a-44b9-4abc-9185-1084046600e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(q_network, target_network):\n",
    "    '''\n",
    "    Update the target network with the weights of the Q-network.\n",
    "\n",
    "    This function synchronizes the target network by copying the weights \n",
    "    from the Q-network. \n",
    "    \n",
    "    In reinforcement learning we want to stabilize \n",
    "    training by keeping the target network fixed for a few steps while \n",
    "    updating the Q-network.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    q_network: The main Keras Q-network model whose weights are being updated frequently.\n",
    "\n",
    "    target_network: The target Keras model to be updated with the weights of the Q-network after a few steps.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    None: The target network is updated in place.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    target_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff6671-47d9-4882-92d3-86cd46f800d6",
   "metadata": {},
   "source": [
    "The next function calculates the reward based on portfolio value change and market conditions. This is the heart of the learning algorithms and by any means we provide the definitive ideal reward function. We tried to balance between instant reward of making a profit every day with finding incentives/penalties given the market conditions.\n",
    "\n",
    "There are many possible fine-tuning for the reward function, we populated as default the values relevant for our example. \n",
    "\n",
    "The relevant inputs are\n",
    "\n",
    "- portfolio: Current portfolio value before taking the action.\n",
    "\n",
    "- positions: Current number of units of stock held.\n",
    "\n",
    "- current_price: The current price of the stock.\n",
    "\n",
    "- action: Integer. The action taken by the agent:\n",
    "            - 0: Buy\n",
    "            - 1: Hold\n",
    "            - 2: Sell\n",
    "\n",
    "- aux_mean: Auxiliary mean, the mean of the previous prices the agent has access to.\n",
    "\n",
    "- aux_std: Auxiliary standard deviation, the std of the previous prices the agent has access to.\n",
    "\n",
    "- scaling_default: (default=0.05). Minimum value for the scaling factor applied to additional incentives and penalties. Generally we want a scale to be the order of the typical daily reward.\n",
    "\n",
    "- incentive_price_mean: (default=1.5). Multiplier for the reward/penalty adjustment based on price deviation from the auxiliary mean.\n",
    "\n",
    "- sell_incentive: (default=1.7). Multiplier to incentive when the agent sells at peak prices.\n",
    "\n",
    "- hold_stable: (default=1.2). Multiplier for rewarding the agent for holding in stable price ranges.\n",
    "\n",
    "- hold_extreme: (default=0.75). Multiplier for penalizing the agent for holding during extreme price deviations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c1fb9-05a1-45f0-a69a-63bab1f3e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(portfolio, positions, current_price, action, aux_mean, aux_std, augmented_state, \\\n",
    "                     scaling_default=0.1,incentive_price_mean=2,sell_incentive=1,hold_stable=0.8,hold_extreme=1):\n",
    "    '''\n",
    "    Calculate the reward based on portfolio value change and market conditions.\n",
    "\n",
    "    This is the heart of the learning algorithms and by any means the definitive ideal reward function.\n",
    "    We tried to balance between instant reward of making a profit every day with finding incentives/penalties given the market conditions.\n",
    "\n",
    "    There are many possible fine-tuning for the reward function, we populated as default the values relevant for our example. \n",
    "    \n",
    "\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "\n",
    "    portfolio: Current portfolio value before taking the action.\n",
    "\n",
    "    positions: Current number of units of stock held.\n",
    "\n",
    "    current_price: The current price of the stock.\n",
    "\n",
    "    action: Integer. The action taken by the agent:\n",
    "            - 0: Buy\n",
    "            - 1: Hold\n",
    "            - 2: Sell\n",
    "\n",
    "    aux_mean: Auxiliary mean, the mean of the previous prices the agent has access to.\n",
    "\n",
    "    aux_std: Auxiliary standard deviation, the std of the previous prices the agent has access to.\n",
    "\n",
    "    augmented_state: \n",
    "\n",
    "    scaling_default: (default=0.05). Minimum value for the scaling factor applied to additional incentives and penalties. Generally we want a scale\n",
    "    to be the order of the typical daily reward.\n",
    "\n",
    "    incentive_price_mean: (default=1.5). Multiplier for the reward/penalty adjustment based on price deviation from the auxiliary mean.\n",
    "\n",
    "    sell_incentive: (default=1.7). Multiplier to incentive when the agent sells at peak prices.\n",
    "\n",
    "    hold_stable: (default=1.2). Multiplier for rewarding the agent for holding in stable price ranges.\n",
    "\n",
    "    hold_extreme: (default=0.75). Multiplier for penalizing the agent for holding during extreme price deviations.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "\n",
    "    reward: The calculated reward, which incorporates portfolio value change and adjustments based on the action and market conditions.\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Calculate portfolio value change\n",
    "    portfolio_new = portfolio + positions * current_price\n",
    "    raw_reward = portfolio_new - portfolio  # Absolute change\n",
    "\n",
    "    # Proportional reward\n",
    "    reward = (raw_reward / portfolio) if portfolio != 0 else 0\n",
    "\n",
    "\n",
    "    # Scaling factor for additional incentives/penalties\n",
    "    scaling_factor = max(scaling_default, abs(reward))\n",
    "\n",
    "\n",
    "    \n",
    "    # Adjust reward based on price deviation from mean\n",
    "    price_deviation = (aux_mean - current_price) / aux_std\n",
    "    reward += scaling_factor * incentive_price_mean * price_deviation\n",
    "\n",
    "\n",
    "    # Action-specific incentives\n",
    "    if action == 2:  # Sell\n",
    "        # Extra incentive for selling at peaks\n",
    "        reward += scaling_factor * sell_incentive * max(0, -price_deviation)\n",
    "        \n",
    "\n",
    "    elif action == 1:  # Hold\n",
    "        # Bonus for holding in a stable price range\n",
    "        holding_bonus = max(0, 1 - abs(price_deviation))\n",
    "        reward += scaling_factor * hold_stable * holding_bonus\n",
    "\n",
    "        \n",
    "\n",
    "        # Penalty for holding during extreme deviations\n",
    "        reward -= scaling_factor * hold_extreme * abs(price_deviation)\n",
    "\n",
    "\n",
    "    elif action == 0:  # Buy\n",
    "        # Incentive for buying during a price dip\n",
    "        momentum = current_price - aux_mean\n",
    "        if momentum < 0:  # Price is below mean\n",
    "            reward += scaling_factor * 1.4\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    # Final condition to make sure we don't simply sell all our positions!\n",
    "    \n",
    "    if positions==0 and action!=0:\n",
    "        reward -= scaling_factor*2\n",
    "        \n",
    "        \n",
    "    return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52f733-391a-4f53-8979-89b45ad673d0",
   "metadata": {},
   "source": [
    "The following is an auxiliary function to train the Q-network. Since `q_network` is a Keras object, it updates its weights in-place, so we do not need to return it.\n",
    "\n",
    "This function samples a batch of past experiences from the replay buffer (custom class we defined earlier) and uses Keras' NN as a function approximator of the Bellman equation. The replay buffer ensures the network learns from diverse experiences.\n",
    "\n",
    "The Q-network uses these experiences to estimate the rewards (Q-values) for different actions. The target network provides stable Q-value targets to prevent oscillations during training. We update the weights of the target network every few episodes.\n",
    "\n",
    "If the Q-network is well-trained, it should accurately approximate the rewards of different actions, enabling optimized decision-making during testing or real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044240a-9ee2-4135-9c53-9787d3835d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_network(q_network, target_network, replay_buffer, batch_size, gamma, seed=SEED):\n",
    "    \"\"\"\n",
    "    Train the Q-network using a batch of experiences from the replay buffer.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    q_network: The current Q-network to be trained. (keras.Model)\n",
    "    \n",
    "    target_network: The target Q-network for stability in training. (keras.Model)\n",
    "    \n",
    "    replay_buffer: Buffer storing past experiences (state, action, reward, next state). (Custom ReplayBuffer class)\n",
    "    \n",
    "    batch_size: The number of experiences to sample for training.\n",
    "    \n",
    "    gamma: Discount factor for future rewards. Parameter in Bellman equation.\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    \n",
    "    list: Updated Q-values for the first sample in the batch.\n",
    "        \n",
    "    loss: Training loss for the batch.\n",
    "    \n",
    "    \"\"\"\n",
    "    #np.random.seed(seed)  \n",
    "    #random.seed(seed)  \n",
    "\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return None, None  # Not enough samples to train\n",
    "\n",
    "    # Sample a batch from the replay buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    states, actions, rewards, next_states = zip(*batch) #zip to \"unzip\" the tuples return by the replay_buffer.sample\n",
    "\n",
    "    # Prepare arrays for training\n",
    "    states = np.array(states)  # Shape: (batch_size, state_dim)\n",
    "    next_states = np.array(next_states)  # Shape: (batch_size, state_dim)\n",
    "\n",
    "    # Predict Q-values for current states and next states\n",
    "    q_values = q_network.predict(states, verbose=0)\n",
    "    next_q_values = target_network.predict(next_states, verbose=0)\n",
    "\n",
    "    # Update Q-values using the Bellman equation\n",
    "    for i in range(batch_size):\n",
    "        q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "\n",
    "    # Train the Q-network\n",
    "    history = q_network.fit(states, q_values, verbose=0)\n",
    "    loss = history.history['loss'][0]\n",
    "\n",
    "    return q_values[0].tolist(), loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5333ac-4a75-4bb0-88ae-e229ab923ead",
   "metadata": {},
   "source": [
    "Since training the Reinforcement Learning model can crash the kernel (specially for more data and longer rolling window), it is important to keep a very detailed log of the episode under consideration.\n",
    "\n",
    "We save the q-network as an h5 file, record the parameters we are at the end of the training, record information about epsilon (as we decrease over episodes in order to change the balance between exploration and exploitation), and also the buffer of states we have visited.\n",
    "\n",
    "If the kernel crashes, we can use this information to resume training as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6ce7aa-e57f-4607-b657-7dfa2457396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_episode_data(episode, q_network, replay_buffer, metrics_df, reward, total_reward, \\\n",
    "                     portfolio, positions, epsilon, epsilon_min, epsilon_decay):\n",
    "    \"\"\"\n",
    "    Log metrics, save the Q-network, and export the replay buffer for a given episode.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    episode: The current episode number. \n",
    "    \n",
    "    q_network: The Q-network to save. (keras.Model)\n",
    "    \n",
    "    replay_buffer: Buffer storing past experiences. (Custom ReplayBuffer class)\n",
    "    \n",
    "    metrics_df: DataFrame to store episode metrics. (pandas.DataFrame)\n",
    "    \n",
    "    reward: Reward for the current step. \n",
    "    \n",
    "    total_reward: Accumulated reward for the episode. \n",
    "    \n",
    "    portfolio: Portfolio value at the end of the episode. \n",
    "    \n",
    "    positions: Number of stocks held at the end of the episode. \n",
    "    \n",
    "    epsilon: Current epsilon value for exploration. \n",
    "    \n",
    "    epsilon_min: Minimum epsilon value for exploration. \n",
    "    \n",
    "    epsilon_decay: Epsilon decay rate per episode. \n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    metrics_df: Updated DataFrame containing metrics for all episodes.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Save Q-network checkpoint\n",
    "    q_network.save(f'2trading_q_network_episode_{episode+1}.h5')\n",
    "\n",
    "    # Save metrics as a new row of a data frame, then concat with the previous log.\n",
    "    new_row = pd.DataFrame([{\n",
    "        'episode': episode,\n",
    "        'reward': reward,\n",
    "        'total_reward': total_reward,\n",
    "        'portfolio': portfolio,\n",
    "        'positions': positions,\n",
    "        'epsilon': epsilon,\n",
    "        'epsilon_min': epsilon_min,\n",
    "        'epsilon_decay': epsilon_decay\n",
    "    }])\n",
    "    metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "    metrics_df.to_csv('metrics_df.csv', index=False)\n",
    "\n",
    "    # Save replay buffer\n",
    "    buffer_df = pd.DataFrame(list(replay_buffer.buffer), columns=['augmented_state', 'action', 'reward', 'next_augmented_state'])\n",
    "    buffer_df.to_csv('replay_buffer.csv', index=False)\n",
    "\n",
    "    return metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2fc45-48bc-491b-8704-27dff0b9e53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
